{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is an analysis of NYC OpenData's \"Inmate Discharges\" dataset. \n",
    "### Disclaimer: Does not included Sealed cases. \n",
    "dataset can be found here: https://data.cityofnewyork.us/Public-Safety/Inmate-Discharges/94ri-3ium\n",
    "\n",
    "help with querying: https://support.socrata.com/hc/en-us/articles/202949268-How-to-query-more-than-1000-rows-of-a-dataset ; https://docs.python-requests.org/en/latest/\n",
    "\n",
    "My goal is to predict how long inmates are held based on the other features in the dataset. I will define \"Time Held\" as (DISCHARGED_DT â€“ ADMITTED_DT)\n",
    "\n",
    "Normalizes the data, tries different models on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNote: inmate_status_code meanings\\n    CS= City Sentenced\\n    CSP= City Sentenced - with VP Warrant\\n    DE= Detainee\\n    DEP= Detainee - with Open Case & VP Warrant\\n    DNS= Detainee- Newly Sentenced to State Time\\n    DPV= Detainee- Technical Parole Violator\\n    SCO= State Prisoner- Court Order\\n    SSR= State Ready\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Note: inmate_status_code meanings\n",
    "    CS= City Sentenced\n",
    "    CSP= City Sentenced - with VP Warrant\n",
    "    DE= Detainee\n",
    "    DEP= Detainee - with Open Case & VP Warrant\n",
    "    DNS= Detainee- Newly Sentenced to State Time\n",
    "    DPV= Detainee- Technical Parole Violator\n",
    "    SCO= State Prisoner- Court Order\n",
    "    SSR= State Ready\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# preprocess categorical data\\ncategorical_cols = ['inmate_status_code']\\ncategorical_transformer = Pipeline(steps=[\\n    ('onehot', OneHotEncoder(handle_unknown='ignore'), drop='first')\\n])\\n\\n# Bundle preprocessing for numerical and categorical data\\npreprocessor = ColumnTransformer(\\n    transformers=[\\n        ('cat', categorical_transformer, categorical_cols)\\n    ])\\n    \\n# drop certain columns from analysis - ['admitted_dt','discharged_dt', 'inmateid', \\n'discharged_dayofweek','discharged_hour', 'gender_F ', 'gender_M ','inmate_status_CSP', \\n'inmate_status_CSP', 'inmate_status_DNS', 'race_BLACK', 'race_UNKNOWN', 'race_ASIAN', \\n'inmate_status_DEP', 'inmate_status_SSR']\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Building a Pipeline'''\n",
    "\n",
    "# one hot encode categorical variables\n",
    "# do that in the pipeline\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# get the data through the API\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# takes url of the dataset, returns a Pandas DataFrame of the dataset\n",
    "def getNYCdata(url):\n",
    "    key = 'g2ZZTkxY9ZQEVVddSnAwJvTdO' # TO DO: this should be secret\n",
    "    response = requests.get(url, headers={'X-App-Token': key})\n",
    "    data = response.text\n",
    "    df = pd.read_json(data)\n",
    "    return df\n",
    "    \n",
    "# Inmate Discharges\n",
    "url = 'https://data.cityofnewyork.us/resource/94ri-3ium.json?$limit=50000&$offset=50000'\n",
    "discharges = getNYCdata(url)\n",
    "\n",
    "# create time held metric\n",
    "discharges['time_held'] = pd.to_datetime(discharges['discharged_dt'])-pd.to_datetime(discharges['admitted_dt'])\n",
    "\n",
    "# drop columns that I don't need\n",
    "discharges = discharges.drop('top_charge', axis=1)\n",
    "discharges = discharges.dropna()\n",
    "# drop duplicates\n",
    "discharges = discharges.drop_duplicates(subset=['inmateid'])\n",
    "\n",
    "discharges = discharges.set_index('inmateid')\n",
    "# discharges = discharges.drop('inmateid', axis=1)\n",
    "\n",
    "# add columns with time of day, time of week admitted/discharged\n",
    "discharges['admitted_dt'] = pd.to_datetime(discharges['admitted_dt'])\n",
    "discharges['discharged_dt'] = pd.to_datetime(discharges['discharged_dt'])\n",
    "\n",
    "discharges['admitted_hour'] = discharges['admitted_dt'].dt.hour\n",
    "discharges['discharged_hour'] = discharges['discharged_dt'].dt.hour\n",
    "\n",
    "discharges['admitted_dayofweek'] = discharges['admitted_dt'].dt.dayofweek\n",
    "discharges['discharged_dayofweek'] = discharges['discharged_dt'].dt.dayofweek\n",
    "\n",
    "# one-hot encode the categorical variables\n",
    "\n",
    "#cat_cols = ['gender', 'inmate_status_code']\n",
    "gender_dummies = discharges['gender'].str.get_dummies()\n",
    "inmate_status_dummies = discharges['inmate_status_code'].str.get_dummies()\n",
    "race_dummies = discharges['race'].str.get_dummies()\n",
    "\n",
    "# use pd.concat to join the new columns with your original dataframe\n",
    "discharges = pd.concat([discharges,pd.get_dummies(discharges['gender'], prefix='gender')],axis=1)\n",
    "discharges = pd.concat([discharges,pd.get_dummies(discharges['race'], prefix='race')],axis=1)\n",
    "discharges = pd.concat([discharges,pd.get_dummies(discharges['inmate_status_code'], prefix='inmate_status')],axis=1)\n",
    "\n",
    "# drop the non-encoded columns\n",
    "discharges = discharges.drop(['race','gender','inmate_status_code'], axis=1)\n",
    "\n",
    "\n",
    "### TO DO: process categorical variables this way eventually\n",
    "\n",
    "'''\n",
    "# preprocess categorical data\n",
    "categorical_cols = ['inmate_status_code']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'), drop='first')\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "    \n",
    "# drop certain columns from analysis - ['admitted_dt','discharged_dt', 'inmateid', \n",
    "'discharged_dayofweek','discharged_hour', 'gender_F ', 'gender_M ','inmate_status_CSP', \n",
    "'inmate_status_CSP', 'inmate_status_DNS', 'race_BLACK', 'race_UNKNOWN', 'race_ASIAN', \n",
    "'inmate_status_DEP', 'inmate_status_SSR']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b6e4383c10ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmi_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmi_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_mi_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmi_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def make_mi_scores(X, y):\n",
    "    mi_scores = mutual_info_regression(X, y)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "mi_scores = make_mi_scores(X, y)\n",
    "\n",
    "mi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW start modeling\n",
    "'''Building a Pipeline - Not using ColumnTransformer b/c it does not help me'''\n",
    "\n",
    "# one hot encode categorical variables\n",
    "# do that in the pipeline\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# get the data through the API\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# takes url of the dataset, returns a Pandas DataFrame of the dataset\n",
    "def getNYCdata(url):\n",
    "    key = 'g2ZZTkxY9ZQEVVddSnAwJvTdO' # TO DO: this should be secret\n",
    "    response = requests.get(url, headers={'X-App-Token': key})\n",
    "    data = response.text\n",
    "    df = pd.read_json(data)\n",
    "    return df\n",
    "    \n",
    "# Inmate Discharges\n",
    "url = 'https://data.cityofnewyork.us/resource/94ri-3ium.json?$limit=50000&$offset=50000'\n",
    "discharges = getNYCdata(url)\n",
    "\n",
    "# create time held metric\n",
    "discharges['time_held'] = pd.to_datetime(discharges['discharged_dt'])-pd.to_datetime(discharges['admitted_dt'])\n",
    "\n",
    "\n",
    "discharges = discharges.set_index('inmateid')\n",
    "# discharges = discharges.drop('inmateid', axis=1)\n",
    "\n",
    "# add columns with time of day, time of week admitted\n",
    "discharges['admitted_dt'] = pd.to_datetime(discharges['admitted_dt'])\n",
    "discharges['admitted_hour'] = discharges['admitted_dt'].dt.hour\n",
    "discharges['admitted_dayofweek'] = discharges['admitted_dt'].dt.dayofweek\n",
    "discharges = discharges.reset_index()\n",
    "\n",
    "# drop duplicates\n",
    "discharges = discharges.drop_duplicates(subset=['inmateid'])\n",
    "\n",
    "# drop columns that I don't need and the nulls\n",
    "discharges = discharges.drop(['top_charge','race','gender', \n",
    "                              'admitted_dt','discharged_dt', 'inmateid'], axis=1)\n",
    "discharges = discharges.dropna()\n",
    "\n",
    "# one-hot encode the categorical variables\n",
    "\n",
    "#cat_cols = ['gender', 'inmate_status_code']\n",
    "inmate_status_dummies = discharges['inmate_status_code'].str.get_dummies()\n",
    "\n",
    "# use pd.concat to join the new columns with your original dataframe\n",
    "discharges = pd.concat([discharges,pd.get_dummies(discharges['inmate_status_code'], prefix='inmate_status')],axis=1)\n",
    "\n",
    "# drop the inmate_status_codes that I'm not using\n",
    "discharges = discharges.drop(['inmate_status_CSP', 'inmate_status_CSP', 'inmate_status_DNS',\n",
    "                              'inmate_status_DEP', 'inmate_status_SSR', 'inmate_status_code'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = discharges.copy()\n",
    "y = X.pop('time_held')\n",
    "\n",
    "# clean up types\n",
    "X['age'] = X['age'].astype('int64')\n",
    "y = y.apply(lambda x: x.value)\n",
    "\n",
    "rs = 302\n",
    "\n",
    "# Normalization\n",
    "#X=(X-np.min(X))/(np.max(X) - np.min(X))\n",
    "\n",
    "from sklearn import preprocessing\n",
    " \n",
    "scaler = preprocessing.StandardScaler().fit(X)   # X is an array with all our features\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=rs)\n",
    "\n",
    "# Random Forest, Decision Tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Function for comparing different approaches\n",
    "def score_dataset(X_valid, y_valid, random_state, model):\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)\n",
    "\n",
    "def printCVScore(scores):\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mean error squared:\n",
    "'time_held' is in nanoseconds, so MAE of 5721308068400939.0 means the estimate was about a week off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-74cb5ddfc7f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mgnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mgnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: fit() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "# RandomForestc\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=rs)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "# Naive Bayes\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "#gnb = GaussianNB\n",
    "#gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next one\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "maes = []\n",
    "for model in [lr, rf, svc, knn]:\n",
    "    print(\"next one\")\n",
    "    mae = score_dataset(X_test, y_test,rs, model)\n",
    "    maes.append(mae)\n",
    "    cvscores = cross_val_score(model, X, y, cv=5)\n",
    "    scores.append(cvscores)\n",
    "    printCVScore(cvscores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this means that the data got predicted correctly about 23 percent of the time? I'm still not sure\n",
    "\n",
    "array([0.21910657, 0.23074301, 0.2204047 , 0.23948893, 0.27446025])\n",
    "\n",
    "Thank you\n",
    "https://stackoverflow.com/questions/52611498/need-help-understanding-cross-val-score-in-sklearn-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printCVScore(scores):\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    \n",
    "printCVScore(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
